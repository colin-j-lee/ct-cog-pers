---
title: "Big Five-Cog ctsem"
format: html
editor: source
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: inline
---

### Working Notes:

Checklist / ToDos
-   [ ] Evan renaming and deleting superfluous code (work in progress)
-   [ ] Data Processing: figure out a strategy for the 'presentation names' of variables. Do we just rename E_scblty to Sociability and keep track of the E=>Soci. mapping? Or do we swap them out prior to making the tables?
-   [x] Hourblock as the dense_rank of sessions within day. (DailySessionNumber might be a better name)
-   [x] Add imputation (at least linear MICE imputation. What about MIDAS imputation? )
-   [ ] Add Time Invariant Predictors
-   [ ] And Time Dependent Predictors
-   [ ] need to merge with baseline data

Notes:
0. I've moved a lot of what was previously in this script to the bottom under "OLDER STUFF", just so we have a pipeline that we can run top to bottom. 
1. I think we'll want to rename the facets to their full, presentation name (E_scblty to Sociability) and keep track of the facet=>domain associations in a named list. Not yet implemented. 
2. we have options for the imputation: predictive mean matching (currently used), weighted predictive mean matching (mice) or something like rMIDAs (a neural network auto-encoder). I haven't checked the pre-reg to see wha we said.
3. 

# Housekeeping


## Packages

```{r}
library(psych)
#library(plyr)
library(mice) # Here we use mice for imputation... but we have enough data for an auto-encoder...
#library(rMIDAS)
#library(ctsem)
library(tidyverse)
library(stringr)
library(flextable)
library(officer)
#library(future)
#availableCores()
#plan(multisession, workers = availableCores() - 1)
#nbrOfWorkers()
```

## Current Date String

```{r}
#to be used for saving plots and tables. if we re-run the pipeline, we just need to be sure we are using the latest dated material.
#the only time this gets slightly tricky is when loading things back in to the pipeline, but we can make a note of that 
CurrentDateString = gsub("\\-", "_",Sys.Date())#replace the dashes with underscores
CurrentDateString
```

## Custom Functions

```{r}
FormatTableToWord = function(Table,OutputFileName) {
  #Initially authored by E.W.
  
  #This formats a table according to basic APA standards, saves it to ms word.
  # in theory, all one has to do is copy and paste into the new document
  #... with less fiddling than would be required by kable()
  #If there is a super complicated custom header structure, kable might be better...
  warning('library(officer)')
  warning('library(flextable)')
  
  big_black_border   <- fp_border(color = "black", width = 2)
  thin_black_border  <- fp_border(color = "black", width = 1)
  light_grey_border  <- fp_border(color = "gray90", width = 1)
  
  my_paragraph = fp_par(
  text.align = 'center',
  padding.top = 1,
  padding.bottom=1,
  padding.left = 2,
  padding.right = 2,
  line_spacing = 1
  )
  
  APATable <- flextable(Table)  |> 
              theme_vanilla() |>
              font(fontname = "Times New Roman", part = "all") |>
              fontsize(size = 12, part = "all") |> 
              bold(part = "header") |> 
              # Remove all existing borders
              border_remove() |> 
              # Add thick border at top of header
              hline_top(part = "header", border = big_black_border) |> 
              hline_bottom(part = "header", border = thin_black_border) |> 
              hline_bottom(part = "body",   border = big_black_border) |> 
              #  Add faint vertical lines (for seeing columns in Word)
              vline(border = light_grey_border, part = "all") |> 
              # ^^ THESE WILL NEED TO BE REMOVED, but are good for fiddling with widths
              set_table_properties(layout = "autofit") |> 
              # Auto-fit column widths for a nice starting point
              autofit() |> 
              #add in styling from above to make everything single spaced with minimal padding
              style(pr_p =   my_paragraph, part = "all")
  
  OutputDoc = read_docx()

  # Add the flextable to the document
  OutputDoc = body_add_flextable(OutputDoc, APATable)

# Save the document
  print(OutputDoc, target = OutputFileName)
  print(paste0('Table Saved as: ',OutputFileName))
  print(paste0("Table Saved to: ", getwd()))
  warning("To remove the vertical borders, go to table design/line styles, select 'no border' and then you have a border eraser")
}

```

## Getting the data
```{r}
#uncomment one of these lines to set your own working directory
#also, this and the next chunk are designed to have the data from Emorie in a top-level subfolder called "DataFromEmorie"

knitr::opts_knit$set(root.dir = '~/Dropbox/Academia/Projects/CTSEMB5StatesCog') #this is for EW's machine
#knitr::opts_knit$set(root.dir = '<ColinsPathStringGoesHere>')

```

### Ema Data

```{r}
ema_df = read_csv("../DataFromEmorie/b5-cog-ema-2025-05-07.csv") 
colnames(ema_df)
head(ema_df)
```

### Baseline Data

```{r}
baseline_df = read_csv("../DataFromEmorie/b5-cog-baseline-2025-05-07.csv") 
colnames(baseline_df)
head(baseline_df)
```

# Data Processing

## EMA Data

1.  Delete EACNO columns; they are the averages of observed values but we need to impute facets then calculate EACNO averages
2.  Create StartDate and HourBlock

```{r}
ema_df = ema_df |> rename(DateTime=Date) |> #First, make the column more informative
  select(-c(A:O)) |> #these are averages of observed values, but we should impute first and then form the averages. 
  select(-StartTime) |> # This looks like the hour of the first day of being in the study? Not sure what it's used for. 
  group_by(SID) |> 
  mutate(StartDate = min(format(DateTime,'%Y:%m:%d'))) |>  #get the start date for each participant
  ungroup() |> 
  group_by(SID,Day) |> 
  mutate(DailySessionNumber = dense_rank(Hour),
         DailyStartTime = min(format(DateTime,"%H:%M:%S"))) |> 
  ungroup()  #something about inconsistent capitalization in the column names

ema_df =  ema_df  |> 
  rename_with(~ str_replace(.x, "^.", ~ toupper(.x)))

ema_df |> head() |> colnames()
```

# Imputation of Big Five Facets

```{r}
Facets = list(
      Extra=c("E_assert","E_scblty","E_enerLev"),
      Agree=c("A_cmpn","A_rspct","A_trust"),
      Consci = c("C_org","C_prdctv","C_rspnsbl"),
      Neuro = c("N_anx","N_dep","N_emoVol"),
      Open = c("O_aesSens","O_crtvImag","O_intCur"))

#sanity check: the correlation matrix checks out:
ema_df |> select(unname(unlist(Facets))) |> cor(use='pairwise.complete.obs') |> round(2)
```

```{r}
#details on mice: https://github.com/amices/mice
ImputedFacets= mice::futuremice(ema_df |> select(unname(unlist(Facets))),
                    m=1,
                    maxit=50,
                    parallelseed = 42,
                    method='pmm', #"Predictive mean matching". details: https://github.com/amices/Winnipeg
                    #method = 'midastouch', #the documentation say that this is "weighted predictive mean matching"
                    #we could also set method='rf' for random forest imptuation. 
                    seed=500)
ema_dfi = complete(ImputedFacets) #ema_dfi ==> ema_DataFrameImputed
colnames(ema_dfi) = unname(unlist(Facets))

ema_df = ema_df |> 
  select(-unname(unlist(Facets))) |> #delete the original columns
  bind_cols(ema_dfi) #glue the two dataframes together along the vertical seam
ema_df

```

### Creating Domain Scores

```{r}

ema_df = ema_df |> 
  bind_cols( #glue together ema_df and whatever comes next along the vertical seam
    map_dfc(Facets, ~rowMeans(ema_df[.x], na.rm = TRUE)) #map_dfc stands for "map data frame column-bind"
    #for each element in Facets (i.e. for each vector of column-name-strings), this gets those columns in ema_df, and takes the row means
  )
ema_df
```


```{r}
warning("TRANSFORM REACTION TIMES / OUTLIER PROCESSING GOES HERE")
```
## R0 Descriptives

### Visualizing a random sample

```{r}
ema_df |>  group_by(SID) %>%
  filter(n() > 40) %>%
  ungroup() %>%
  filter(SID %in% sample(unique(.$SID), 20)) %>%
  ggplot(aes(x = Session, y = Dsm_score)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth() + 
    facet_wrap(~SID, nrow = 4) + 
    theme_bw()
```

5.  R0 (Descriptives, Reliabilities)
    a.  reliabilities, means, sd, corr matrices, baseline descriptives).
    b.  Use omega (use multilevel omega c.f. Colin)
    c.  Use Evan's descriptives function?
    
EW note: this aren't finalized; once we get this sorted out we can either a) feed to Kable or b) feed to the Officer/Flextable function above.
```{r}
CurrentDateString
warning("These are descriptives for the EMA data. I assume we do no pooling within individuals first.")

BasicStatsEma = ema_df |> select(Nback_score:Dsm_medianRTc,
                E_assert:Open) |> 
                psych::describe() |> 
                as.data.frame() |> #psych helpfully returns a non-dataframe object
                select(-c(vars,trimmed,mad,skew,kurtosis,se)) |> 
                 rownames_to_column(var = "Variable")

BasicStatsEma
```

```{r}
#now, export to word
FormatTableToWord(BasicStatsEma,paste0("CT_CogB5_BasicStatsEMA_",CurrentDateString,".docx"))
```

```{r}
CorrTable = ema_df |> select(Nback_score:Dsm_medianRTc,
                E_assert:Open) |> cor(use='pairwise.complete.obs') |> round(2)

warning('Need to figure out how to format this for MS Word.')
#Option 1: Outcomes w/ Outcomes, Pers with Pers.?
#Option 2: Break it up over multiple pages. 
CorrTable 
```
```{r}

#this is an old function I have from a different script.
#omega_total <- if (ncol(subset_data) >= 3) {
#      omega(subset_data,nfactors = NumberOfFacets, check.keys=FALSE)$omega.tot
#    } else {
#      NA
#    }
#    
#    # Cronbach's alpha
#    alpha_reliability <- if (ncol(subset_data) >= 2) {
#      alpha(subset_data,check.keys=FALSE)$total$raw_alpha
#    } else {
#      NA
#    }
#    
#    # Item-total correlations
#    item_total_cor <- if (ncol(subset_data) >= 2) {
#      apply(subset_data, 2, function(x) cor(x, rowSums(subset_data), use = "complete.obs"))
#   } else {
#      rep(NA, ncol(subset_data))
#    }
#    
#    # Add omega, alpha, item-total correlations, and person_wave info to descriptives data frame
#    descriptives <- descriptives %>%
#      mutate(
#        omega_total = omega_total,
#        alpha_reliability = alpha_reliability,
#        item_total_cor = item_total_cor,
#        person_wave = paste0(prefix, wave)
#      )
#    

```
# OLDER STUFF

##Code Emorie used to pull the data?

```{r}
pp_cb <- readxl::read_excel("Codebooks/01-codebook.xlsx", sheet = "codebook")
b5_items <- pp_cb %>% filter(Inventory == "BFI-2") %>% pull(old)
# sc_df <- sc_df %>%
#   #rename(N05 = N5) %>%
#   select(SID, Date = Date_main, session:HourBlock, interrupt, all_of(b5_items)) %>%
#   pivot_longer(
#     cols = all_of(b5_items)
#     , values_to = "value"
#     , names_to = "old"
#     , values_drop_na = T
#   ) %>%
#   left_join(pp_cb %>% select(old, shortFacet, Reverse)) %>%
#   mutate(value = ifelse(Reverse == "yes", 6 - value, value)) %>%
#   separate(old, c("trait", "item"), sep = -2) %>%
#   group_by(SID, Date, StartDate, Day, Hour, HourBlock, session, interrupt, trait, shortFacet) %>%
#   summarize(value = mean(value, na.rm = T)) %>% 
#   ungroup()

#pomp <- function(x, mini = 1, maxi = 5) (x - mini)/(maxi - mini)*100

sc_b5 <- sc_df %>% 
  group_by(SID, Date, StartDate, Day, Hour, HourBlock, session,  interrupt, trait) %>% 
  summarize(value = mean(value, na.rm = T)) %>% 
  ungroup() %>%
  pivot_wider(
    names_from = "trait"
    , values_from = "value"
    ) %>%
  full_join(
    sc_b5 %>%
      pivot_wider(
        names_from = c("trait", "shortFacet")
        , values_from = "value"
        )
  ) #%>% 
  # mutate_at(vars(A:N_emoVol), pomp)
```

## Cognitive States

```{r}
#pomp_obs <- function(x) (x - min(x, na.rm = T))/(max(x, na.rm = T)-min(x, na.rm = T))*100
# sc_cog <- sc_df %>% 
#   select(
#     SID, Date = Date_main, session:HourBlock, interrupt, 
#          , nback_score, vpa_score, dsm_score
#          , nback_medianRTc, vpa_medianRTc, dsm_medianRTc
#          ) %>%
  # mutate_at(vars(nback_score, vpa_score, dsm_score), pomp_obs) %>%
  # rowwise() %>%
  # mutate(cog_comp = mean(c_across(nback_score:dsm_score), na.rm = T)) %>%
  ungroup() 

sc_b5cog <- sc_b5 %>%
  inner_join(
    sc_cog 
    ) %>%
  select(-StartDate, -HourBlock) %>%
  arrange(SID, Date) %>%
  group_by(SID) %>%
  mutate(
    beep = 1:n()
    , beep_wpc = beep - median(beep)
    , dec_time = hour(Date) + minute(Date)/60 + second(Date)/60/60
    , dec_time_c = dec_time - 12
  )
save(sc_b5cog, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-ema-%s.RData", Sys.Date()))
write_csv(sc_b5cog, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-ema-%s.csv", Sys.Date()))
```

```{r}
set.seed(420)
sc_cog %>%
  group_by(SID) %>%
  filter(n() > 40) %>%
  ungroup() %>%
  filter(SID %in% sample(unique(.$SID), 20)) %>%
  ggplot(aes(x = session, y = dsm_score)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth() + 
    facet_wrap(~SID, nrow = 4) + 
    theme_bw()
  
```

# Baseline Data

## Big Five Traits

```{r}
bl_df <- read_csv("parsed-data/baseline/baseline_wide_clean_data_W1_2025-04-30.csv") %>%
  select(
    SID, StartDate = date
    , starts_with("demo_eth"),
    , demo_gender, demo_gender_TEXT = demo_gender_3_TEXT, demo_hispanic
    , demo_orientation, demo_orientation_TEXT = demo_orientation_4_TEXT
    , demo_YOB, demo_age
    , demo_education
    , demo_income
    , demo_relationship 
    # , demo_livingSituation
    # , demo_employment 
    # , demo_mom_edu, demo_dad_edu
    , starts_with("BFI")
    # , health_physAct = `health-physAct` 
    # , health_smoke = `health-smoke`
    # , health_height = `health-height`, health_weight = `health-weight`
    # , health_cholesterol = `health-CC_1`
    # , health_hypertension = `health-CC_2`
    # , health_diabetes = `health-CC_3`
    # , health_depression = `health-CC_4`
    # , health_tbi = `health-CC_5`
    # , health_heartDis = `health-CC_6`
    # , health_renal = `health-CC_7`
    # , starts_with("sud")
    ) %>%
  filter(!is.na(SID))
write_csv(bl_df, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-baseline-%s.csv", Sys.Date()))
save(bl_df, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-baseline-%s.RData", Sys.Date()))
```

## Ctsem shite

```{r, include = FALSE}
library(rstan)
library(ctsem)
library(tidybayes)
library(furrr)
library(plyr)
library(tidyverse)
```

## Testing

simple ass test

```{r}
model<-ctModel(type='stanct'
            , n.latent = 2, latentNames   = c("E", "vpa_score")
            , n.manifest = 2, manifestNames = c("E", "vpa_score")
            , TIpredNames = c("E_baseline", "age")
            , TDpredNames = c("interrupt", "Hour", "session")
            , LAMBDA=diag(2)
            , MANIFESTMEANS = 0 #fix this for estimation 
            , CINT = matrix(c("mu_trait", "mu_cog"), nrow = 2) #estimate this for equil calculation
            , DRIFT = matrix(c("beta_trait", "beta_trait_on_cog", "beta_cog_on_trait", "beta_cog"), nrow = 2, byrow = T)
            #diagonal for estimation
            , MANIFESTVAR   = matrix(c("mv_trait", 0, 0, "mv_cog"), nrow = 2, byrow = TRUE)
            , DIFFUSION = matrix(c("diff_trait", 0, 0, "diff_cog"), nrow = 2, byrow = TRUE), 
            , T0VAR = matrix(c("var_trait", 0, 0, "var_cog"), nrow = 2, byrow = TRUE) 
            , id            = "id"
            , time          = "time" )

model$pars$indvarying[7:14] <- TRUE #drift and diffusion
model$pars$transform[11] <- "log1p_exp(2 * param) + 1e-10" #drift
model$pars$transform[14] <- "log1p_exp(2 * param) + 1e-10" #diffusion
model$pars$transform[21:22] <- "1*param" #CINT

model$pars



model_fit <- ctStanFit(datalong = sherman_wide_person_centered, 
                      ctstanmodel = model,
                      optimize = TRUE,
                      priors = TRUE,
                      cores=10
                      # ,
                      # optimcontrol=list(is=TRUE, 
                      #                   finishsamples = 1000, 
                      #                   isloopsize = 2000)
                      )

# Summary of the model
modelsum <- summary(model_fit, digits = 4, parmatrices=TRUE)
modelpar <- modelsum$parmatrices
print(modelsum$parmatrices)

```

All 30 trait-cog pairs

```{r}
# Define traits and cogs
traits <- colnames(sc_df)[4:8] #change col numbers
cogs <- colnames(sc_df)[9:16] #change col numbers

# Create all trait-cog pairs
trait_cog_pairs <- expand_grid(trait = traits, cog = cogs)

fit_bivariate_model <- function(trait, cog) {
  message("Fitting model for:", trait, "and", cog, "\n")

  tryCatch({
    var_names <- c(trait, cog)

    model <- ctModel(
      type = 'stanct',
      n.latent = 2,
      latentNames = var_names,
      n.manifest = 2,
      manifestNames = var_names,
      LAMBDA = diag(2),
      
      TDpred
      
      MANIFESTMEANS = 0,

      CINT = matrix(c(
        paste0("mu_", trait),
        paste0("mu_", cog)
      ), nrow = 2),

      DRIFT = matrix(c(
        paste0("beta_", trait),
        paste0("beta_", trait, "_on_", cog),
        paste0("beta_", cog, "_on_", trait),
        paste0("beta_", cog)
      ), nrow = 2, byrow = TRUE),

      MANIFESTVAR = matrix(c(
        paste0("mv_", trait), 0,
        0, paste0("mv_", cog)), 
        nrow = 2, byrow = TRUE),

      DIFFUSION = matrix(c(
        paste0("diff_", trait), 0,
        0, #lower diagonal f's stuff up
        paste0("diff_", cog)
      ), nrow = 2, byrow = TRUE),

      T0VAR = matrix(c(
        paste0("t0var_", trait), 0,
        0, paste0("t0var_", cog)), 
        nrow = 2, byrow = TRUE),

      id = "id",
      time = "time"
    )

    model$pars$indvarying[7:14] <- TRUE #drift and diffusion
    
    #make diffusion variances reasonable
    model$pars$transform[11] <- "log1p_exp(2 * param) + 1e-10"
    model$pars$transform[14] <- "log1p_exp(2 * param) + 1e-10"

    data_sub <- sherman_wide_person_centered %>%
      select(id, time, all_of(c(trait, cog)))

    fit <- ctStanFit(datalong = data_sub, 
                     ctstanmodel = model, 
                     optimize = TRUE, 
                     priors = TRUE,
                     cores = 2, 
                     verbose = 1
                     )

    fit_summary <- summary(fit, digits = 4, parmatrices = TRUE)

    tibble(
      pair = paste0(trait, "_", cog),
      model = list(model),
      fit = list(fit),
      summary = list(fit_summary)
    )
  }, error = function(e) {
    cat("Error in model for: ", trait, "_", cog, "\n", e$message)  # Log the error
    tibble(
      pair = paste0(trait, "_", cog),
      model = list(NA),
      fit = list(NA),
      summary = list(e$message)
    )
  })
}

# Parallel plan
plan(multisession, workers = 4)

# Fit all models
ml_bivariate_results <- future_pmap_dfr(trait_cog_pairs, fit_bivariate_model, .progress = TRUE)
#check
# ml_bivariate_results$summary[3][[1]]$parmatrices

# Save results
save(ml_bivariate_results, file = "Data/ml_bivariate_results.RData")

# Return to sequential plan
plan(sequential)

q()

```

Extract parameters and find out which didn't converge and reestimate

Yo we good - 5/14/25

```{r}
# #load("Data/ml_bivariate_results.RData")
# 
# ml_bivariate_summary_t0_fixed <- ml_bivariate_results %>%
#   select(pair, summary)
# 
# # Extract the parmatrices from each summary
# ml_bivariate_parmatrices_t0_fixed <- ml_bivariate_summary_t0_fixed %>%
#   mutate(parmatrices = map(summary, ~ .x$parmatrices)) %>%
#   select(pair, parmatrices)
# 
# #Are any estimates hella big; this is  indicative of poor model convergence and I can separately estimate them
# 
# par_check_t0_fixed <- ml_bivariate_parmatrices_t0_fixed %>%
#   mutate(any_mean_gt_20 = map_lgl(parmatrices, ~ any(.x$Mean > 20, na.rm = TRUE)))
# 
# par_large_means_t0_fixed <- par_check_t0_fixed %>%
#   filter(any_mean_gt_20) %>%
#   mutate(high_mean_rows = map(parmatrices, ~ filter(.x, Mean > 20)))
# 
# par_large_unnest_t0_fixed <- par_large_means_t0_fixed %>% unnest(high_mean_rows) %>%
#   mutate(Mean = round(Mean, 3))
# 
# #im so fuckin happy
```
