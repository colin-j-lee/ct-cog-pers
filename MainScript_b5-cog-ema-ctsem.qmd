---
title: "Big Five-Cog ctsem"
format: html
editor: source
  markdown: 
    wrap: 72
editor_options: 
  chunk_output_type: inline
---

# Working Notes:

Checklist / ToDos
-   [ ] Evan renaming and deleting superfluous code (work in progress)
-   [ ] Data Processing: figure out a strategy for the 'presentation names' of variables. Do we just rename E_scblty to Sociability and keep track of the E=>Soci. mapping? Or do we swap them out prior to making the tables?
-   [x] Hourblock as the dense_rank of sessions within day. (DailySessionNumber might be a better name)
-   [x] Add imputation (at least linear MICE imputation. What about MIDAS imputation? )
-   [ ] Add Time Invariant Predictors
-   [ ] And Time Dependent Predictors
-   [ ] need to merge with baseline data

Notes:
0. I've moved a lot of what was previously in this script to the bottom under "OLDER STUFF", just so we have a pipeline that we can run top to bottom.    
1. I think we'll want to rename the facets to their full, presentation name (E_scblty to Sociability) and keep track of the facet=>domain associations in a named list. Not yet implemented. 
2. We have options for the imputation: predictive mean matching (currently used), weighted predictive mean matching (mice) or something like rMIDAs (a neural network auto-encoder). I haven't checked the pre-reg to see wha we said.
3. 

# Housekeeping


## Packages

```{r}
library(psych)
library(kableExtra)
#library(plyr)
#library(mice) # Here we use mice for imputation... but we have enough data for an auto-encoder...
library(Amelia) # Amelia rather than mice for continuity
#library(rMIDAS)
library(ctsem)
library(stringr)
library(officer)
library(future)
library(furrr)
library(tidyverse)
library(flextable)
library(officer)
```

```{r}
availableCores()
plan(multisession, workers = availableCores() - 1)
nbrOfWorkers()
```
## Current Date String

```{r}
#to be used for saving plots and tables. if we re-run the pipeline, we just need to be sure we are using the latest dated material.
#the only time this gets slightly tricky is when loading things back in to the pipeline, but we can make a note of that 
CurrentDateString = gsub("\\-", "_",Sys.Date())#replace the dashes with underscores
CurrentDateString
```

## Custom Functions

```{r}
FormatTableToWord = function(Table, OutputFileName, table_number = NULL, caption = NULL) {
  #Initially authored by E.W.
  
  #This formats a table according to basic APA standards, saves it to ms word.
  # in theory, all one has to do is copy and paste into the new document
  #... with less fiddling than would be required by kable()
  #If there is a super complicated custom header structure, kable might be better...
  warning('library(officer)')
  warning('library(flextable)')
  
  big_black_border   = fp_border(color = "black", width = 2)
  thin_black_border  = fp_border(color = "black", width = 1)
  light_grey_border  = fp_border(color = "gray90", width = 1)
  
  my_paragraph = fp_par(
    text.align = 'center',
    padding.top = 1,
    padding.bottom=1,
    padding.left = 2,
    padding.right = 2,
    line_spacing = 1
  )
  
  APATable = flextable(Table)  |> 
    theme_vanilla() |>
    font(fontname = "Times New Roman", part = "all") |>
    fontsize(size = 12, part = "all") |> 
    bold(part = "header") |> 
    # Remove all existing borders
    border_remove() |> 
    # Add thick border at top of header
    hline_top(part = "header", border = big_black_border) |> 
    hline_bottom(part = "header", border = thin_black_border) |> 
    hline_bottom(part = "body",   border = big_black_border) |> 
    #  Add faint vertical lines (for seeing columns in Word)
    vline(border = light_grey_border, part = "all") |> 
    # ^^ THESE WILL NEED TO BE REMOVED, but are good for fiddling with widths
    set_table_properties(layout = "autofit") |> 
    # Auto-fit column widths for a nice starting point
    autofit() |> 
    #add in styling from above to make everything single spaced with minimal padding
    style(pr_p = my_paragraph, part = "all")
  
  OutputDoc = read_docx()
  

  
 # Add table number (bold) if provided
  if (!is.null(table_number)) {
    OutputDoc = body_add_fpar(OutputDoc, 
                              fpar(ftext(table_number, 
                                        fp_text(font.family = "Times New Roman", 
                                               font.size = 12, 
                                               bold = TRUE))))
  }
  
  # Add caption (not bold) if provided
  if (!is.null(caption)) {
    OutputDoc = body_add_fpar(OutputDoc,
                              fpar(ftext(caption,
                                        fp_text(font.family = "Times New Roman", 
                                               font.size = 12, 
                                               bold = FALSE))))
  }
  
  # Add the flextable to the document
  OutputDoc = body_add_flextable(OutputDoc, APATable)
  
  # Save the document
  print(OutputDoc, target = OutputFileName)
  print(paste0('Table Saved as: ', OutputFileName))
  print(paste0("Table Saved to: ", getwd()))
  warning("To remove the vertical borders, go to table design/line styles, select 'no border' and then you have a border eraser")
}
```

## Getting the data
```{r}
#uncomment one of these lines to set your own working directory
#also, this and the next chunk are designed to have the data from Emorie in a top-level subfolder called "DataFromEmorie"

knitr::opts_knit$set(root.dir = '~/Dropbox/Academia/Projects/CTSEMB5StatesCog') #this is for EW's machine
#knitr::opts_knit$set(root.dir = '~/Downloads/Projects/Cog States Continuous Time')
#knitr::opts_knit$set(root.dir = '~/Desktop/BeckLabRAStuff/CTSEM/') #for HW's machine
```

### Ema Data

```{r}
ema_df = read_csv("../DataFromEmorie/b5-cog-ema-2025-05-07.csv")  #EW
#ema_df = read_csv("~/Downloads/Projects/Cog States Continuous Time/Data/b5-cog-ema-2025-05-07.csv") #CL
#colnames(ema_df)
#head(ema_df)
#ema_df |> arrange(SID)
```

### Baseline Data

```{r}
baseline_df = read_csv("../DataFromEmorie/b5-cog-baseline-2025-05-07.csv") #EW
#baseline_df = read_csv("~/Downloads/Projects/Cog States Continuous Time/Data/b5-cog-baseline-2025-05-07.csv") #CL

colnames(baseline_df)
head(baseline_df)
```

# Data Processing

## EMA Data

1.  Delete EACNO columns; they are the averages of observed values but we need to impute facets then calculate EACNO averages
2.  Create StartDate and HourBlock
3.  There are duplicate rows for some reason, the BFI and cog values are exactly the same; delete em
```{r}
head(ema_df)
```
### Helper Function for converting time to seconds
Should we do hours to make it similar to the time column?

```{r}
ConvertToSecondsRelativeToNoon = function(dt) {
  h = as.numeric(format(dt, "%H"))
  m = as.numeric(format(dt, "%M"))
  s = as.numeric(format(dt, "%S"))
  total_seconds = h * 3600 + m * 60 + s
  return(total_seconds - (12 * 3600))
}
#testdate= ema_df |> select(Date) |> slice(10) |> pull()
#testdate
#ConvertToSecondsRelativeToNoon(testdate)
#ema_df |> arrange(SID,Date) 
```

```{r}
ema_df = ema_df |> 
  arrange(SID,Date) |> #First, ensure that the dataframe is sorted properly (needed for the dense_rank functions)
  rename(DateTime=Date) |> #Second, make sure the column more informative, and make sue 
  mutate(DateTime = ymd_hms(DateTime)) %>%
  select(-c(A:O)) |> #Third, Remove the Domain Scores (averages of observed values) as we should impute first and then form the averages. 
  select(-StartTime) |> # This looks like the hour of the first day of being in the study? Not sure what it's used for. 
  #Next we create two within-person, across all sessions columns:
  group_by(SID) |> 
  mutate(StartDate = min(DateTime, na.rm = TRUE),#get the start date for each participant
         BeepBoops = dense_rank(DateTime), #probably should rename, but this is rank of each measurement occasion within each person. "Beep number" 
         time = as.numeric(difftime(DateTime, StartDate, units = "hours"))) |> 
  ungroup() |> 
  #Now, for each person, for each day:
  group_by(SID,Day) |> 
  mutate(DailySessionNumber = dense_rank(Hour),
         DailyStartTime = min(format(DateTime,"%H:%M:%S")),
         SecondsRelativeToNoon=ConvertToSecondsRelativeToNoon(DateTime)) |> #note that Emorie said something about "decimal time" so maybe we should divide by 10 or 100 here?
  ungroup()

```

```{r}
#next, fix the column name casing, and reorder the columns
ema_df = ema_df |>   rename_with(~ str_replace(.x, "^.", ~ toupper(.x))) |> 
  #now we reorder the columns
  select(SID,Time,DateTime,Day,Hour,Session,DailySessionNumber,BeepBoops,StartDate,DailyStartTime,SecondsRelativeToNoon,Interrupt,
                      everything()) 

#min 25 obs
ema_df <- ema_df %>%
  group_by(SID) %>%
  filter(n() >= 20) %>%
  ungroup()

#remove duplicates
ema_df = ema_df[!duplicated(ema_df), ] #check it out, non tidyverse that's more succinct and readable than tidyverse's version!
```

### Imputation of Big Five Facets
```{r}

ema_df_mi = data.frame(unclass(ema_df %>% 
                                       select(SID, BeepBoops, A_cmpn:N_emoVol)))

bounds_matrix = cbind(3:17, lower = 1, upper = 5)
```

```{r}
ema_df_mi = amelia(ema_df_mi, m = 1, ts = "BeepBoops", cs = "SID", bounds = bounds_matrix)$imputations[[1]] %>%
  as_tibble() %>%
  full_join(ema_df %>% select(SID, BeepBoops, DateTime, Day, Hour, Session, DailySessionNumber, DailyStartTime, SecondsRelativeToNoon, Interrupt), by = c("SID", "BeepBoops")) %>%
  select(-BeepBoops); ema_df_mi

# look at state values post MI
cog_pers = ema_df_mi %>%
  select(A_cmpn:N_emoVol)
describe(cog_pers) 

ema_df = ema_df_mi %>%
  select(SID, DateTime, Day, Hour, Session, DailySessionNumber, DailyStartTime, SecondsRelativeToNoon, Interrupt,
         A_cmpn:N_emoVol)

```

Append baseline variables we want
```{r}

#what happens with baseline_append? don't we want a join?
#relevant vars
baseline_append <- baseline_df %>%
  select(SID, demo_age, contains("BFI"))

# Find SIDs in ema_df that are not in baseline_df
setdiff(ema_df$SID, baseline_append$SID)

```



## Some descriptives
```{r}

#helper data structures
Facets = list(
      Extra=c("E_assert","E_scblty","E_enerLev"),
      Agree=c("A_cmpn","A_rspct","A_trust"),
      Consci = c("C_org","C_prdctv","C_rspnsbl"),
      Neuro = c("N_anx","N_dep","N_emoVol"),
      Open = c("O_aesSens","O_crtvImag","O_intCur"))

ShortToLongNames = c(
  'Extra'='Extraversion',
  'Agree'='Agreeableness',
  'Consci'='Conscientiousness',
  'Neuro'='Neuroticism',
  'Open' = 'Openness'
)

#sanity check: the correlation matrix checks out:
ema_df |> select(unlist(unname(Facets))) |> cor(use='pairwise.complete.obs') |> round(2)
```

# Multilevel Reliability
Note: There are some no variance variables within person
```{r, warning = FALSE}
esm_facets = ema_df %>% select(SID, A_cmpn:N_emoVol)

#im so sorry emorie
# => i've attempted to save our souls with the version of the code below. I created a function to that calculates these omegas, and then I map-apply that over the names of the facts
```

```{r}
#I've kept the warnings unsuprressed just so we can see what happens here
GetOmegaSEM = function(facet_name) {
  FullName = ShortToLongNames[facet_name]
  Omegas = multilevelTools::omegaSEM(data = esm_facets,
                            id = "SID",
                            items = Facets[[facet_name]],
                            savemodel = TRUE
                            )
  Omegas = Omegas$Results |> mutate(trait = rep(FullName ,2),
         level = c("Within-person", "Between-person"),
         estimate = sprintf("%2.2f [%2.2f, %2.2f]", est, ci.lower, ci.upper))
  
 return(Omegas)
}

Reliabilities = bind_rows(future_map(names(Facets), GetOmegaSEM)) #do this in parallel so it's faster
```

```{r}
ReliabilityTable = Reliabilities |> 
  select(trait,level,estimate) |>
  rename(Trait=trait,Type=level, Omega=estimate) |> 
  pivot_wider(names_from = Type, values_from = Omega) 


#This should be fewer clicks to get this table into production / final shape
FormatTableToWord(ReliabilityTable,
                  paste0("MultilevelReliabilityTableEsmData",CurrentDateString,'.docx'),
                  'Table 2',
                  'Multilevel Omega Reliabilities of Momentary BFI-2 Traits'
                  )

# but if you want to kable this:
ReliabilityTable |> 
  kable(format = "html",
        escape = F, align = c("l", "c", "c"),
        caption = "<strong> Table 2 </strong><br><em>Multilevel Omega Reliabilities of Momentary BFI-2 Traits</em>") %>%
  kable_classic(full_width = F, html_font = "Times")
  

```



```{r}


#ml_rel_e= multilevelTools::omegaSEM(data = esm_facets,
#                   id = "SID",
#                   items = Facets[["Extra"]],
#                   savemodel = TRUE
#                   )

#ml_rel_a = multilevelTools::omegaSEM(data = esm_facets,
#                   id = "SID",
#                   items = Facets[["Agree"]],
#                   savemodel = TRUE
#                   )

#ml_rel_c = multilevelTools::omegaSEM(data = esm_facets,
#                   id = "SID",
#                   items = Facets[["Consci"]],
#                   savemodel = TRUE
#                   )

#ml_rel_n = multilevelTools::omegaSEM(data = esm_facets,
#                   id = "SID",
#                   items = Facets[["Neuro"]],
#                   savemodel = TRUE
#                  )

#ml_rel_o = multilevelTools::omegaSEM(data = esm_facets,
#                   id = "SID",
#                   items = Facets[["Open"]],
#                   savemodel = TRUE
#                   )
```
#aggregate into a table; this is fuckin cooked

```{r}
#ml_rel_e_df = ml_rel_e$Results %>%
#  mutate(trait = rep("Extraversion",2),
#         level = c("Within-person", "Between-person"),
#         estimate = sprintf("%2.2f [%2.2f, %2.2f]", est, ci.lower, ci.upper)) %>%
#  select(trait, estimate, level) %>%
#  pivot_wider(names_from = level,
#              values_from = estimate)

#ml_rel_a_df = ml_rel_a$Results %>%
#  mutate(trait = rep("Agreeableness",2),
#         level = c("Within-person", "Between-person"),
#         estimate = sprintf("%2.2f [%2.2f, %2.2f]", est, ci.lower, ci.upper)) %>%
#  select(trait, estimate, level) %>%
#  pivot_wider(names_from = level,
#              values_from = estimate)

#ml_rel_c_df = ml_rel_c$Results %>%
#  mutate(trait = rep("Conscientiousness",2),
#         level = c("Within-person", "Between-person"),
#        estimate = sprintf("%2.2f [%2.2f, %2.2f]", est, ci.lower, ci.upper)) %>%
#  select(trait, estimate, level) %>%
#  pivot_wider(names_from = level,
 #             values_from = estimate)

#ml_rel_n_df = ml_rel_n$Results %>%
#  mutate(trait = rep("Neuroticism",2),
#         level = c("Within-person", "Between-person"),
#         estimate = sprintf("%2.2f [%2.2f, %2.2f]", est, ci.lower, ci.upper)) %>%
#  select(trait, estimate, level) %>%
#  pivot_wider(names_from = level,
#              values_from = estimate)

#ml_rel_o_df = ml_rel_o$Results %>%
#  mutate(trait = rep("Openness",2),
#         level = c("Within-person", "Between-person"),
#         estimate = sprintf("%2.2f [%2.2f, %2.2f]", est, ci.lower, ci.upper)) %>%
#  select(trait, estimate, level) %>%
#  pivot_wider(names_from = level,
#              values_from = estimate)3

# ml_rel_df = rbind(ml_rel_e_df, ml_rel_a_df, ml_rel_c_df, ml_rel_n_df, ml_rel_o_df) %>%
#   rename(Trait = trait)
# colnames(ml_rel_df)[2] = "Within-person <br> (Range)"
# colnames(ml_rel_df)[3] = "Between-person <br> (Range)"
# 
# #lol fuck
# kable(ml_rel_df, format = "html",
#       escape = F, align = c("r", "c", "c"),
#         caption = "<strong> Table 3 </strong><br><em>Multilevel Omega Reliabilities of Momentary BFI-2 Traits</em>") %>%
#     kable_classic(full_width = F, html_font = "Times") 
#   #%>% save_kable(file = "Manuscript Tables and Figures/Multilevel Reliabilities For Each Study.html")
# 

```

```{r}
# #details on mice: https://github.com/amices/mice
# #amelia
# #Colin to Paste amelia / melia imputation here
# #
# ImputedFacets= mice::futuremice(ema_df |> select(unname(unlist(Facets))),
#                     m=1,
#                     maxit=50,
#                     parallelseed = 42,
#                     method='pmm', #"Predictive mean matching". details: https://github.com/amices/Winnipeg
#                     #method = 'midastouch', #the documentation say that this is "weighted predictive mean matching"
#                     #we could also set method='rf' for random forest imptuation. 
#                     seed=500)
# ema_dfi = complete(ImputedFacets) #ema_dfi ==> ema_DataFrameImputed
# colnames(ema_dfi) = unname(unlist(Facets))
# 
# ema_df = ema_df |> 
#   select(-unname(unlist(Facets))) |> #delete the original columns
#   bind_cols(ema_dfi) #glue the two dataframes together along the vertical seam
# ema_df

```

### Creating Domain Scores From Facets

```{r}
ema_df = ema_df |> 
  bind_cols( #glue together ema_df and whatever comes next along the vertical seam
    map_dfc(Facets, ~rowMeans(ema_df[.x], na.rm = TRUE)) #map_dfc stands for "map data frame column-bind"
    #for each element in Facets (i.e. for each vector of column-name-strings), this gets those columns in ema_df, and takes the row means
  )
ema_df
```


```{r}
warning("TRANSFORM REACTION TIMES / OUTLIER PROCESSING GOES HERE")
#colin to take a look
#emorie has code for this
```
## Basline Data
### Item Reversals
```{r}
#do the items need to be reverse scored?
baseline_df |> select(contains("E_")) |> cor(use='pairwise.complete.obs') |> round(2)
#Yes they do!
```
```{r}
Reversals =  list(
  BFI_E_1 = FALSE,
  BFI_E_2 = FALSE,
  BFI_E_3 = TRUE,
  BFI_E_4 = TRUE,
  BFI_E_5 = FALSE,
  BFI_E_6 = FALSE,
  BFI_E_7 = TRUE,
  BFI_E_8 = TRUE,
  BFI_E_9 = FALSE,
  BFI_E_10 = FALSE,
  BFI_E_11 = TRUE,
  BFI_E_12 = TRUE,
  BFI_A_1 = FALSE,
  BFI_A_2 = FALSE,
  BFI_A_3 = TRUE,
  BFI_A_4 = TRUE,
  BFI_A_5 = FALSE,
  BFI_A_6 = FALSE,
  BFI_A_7 = TRUE,
  BFI_A_8 = TRUE,
  BFI_A_9 = FALSE,
  BFI_A_10 = FALSE,
  BFI_A_11 = TRUE,
  BFI_A_12 = TRUE,
  BFI_C_1 = FALSE,
  BFI_C_2 = FALSE,
  BFI_C_3 = TRUE,
  BFI_C_4 = TRUE,
  BFI_C_5 = FALSE,
  BFI_C_6 = FALSE,
  BFI_C_7 = TRUE,
  BFI_C_8 = TRUE,
  BFI_C_9 = FALSE,
  BFI_C_10 = FALSE,
  BFI_C_11 = TRUE,
  BFI_C_12 = TRUE,
  BFI_N_1 = FALSE,
  BFI_N_2 = FALSE,
  BFI_N_3 = TRUE,
  BFI_N_4 = TRUE,
  BFI_N_5 = FALSE,
  BFI_N_6 = FALSE,
  BFI_N_7 = TRUE,
  BFI_N_8 = TRUE,
  BFI_N_9 = FALSE,
  BFI_N_10 = FALSE,
  BFI_N_11 = TRUE,
  BFI_N_12 = TRUE,
  BFI_O_1 = FALSE,
  BFI_O_2 = FALSE,
  BFI_O_3 = TRUE,
  BFI_O_4 = TRUE,
  BFI_O_5 = FALSE,
  BFI_O_6 = FALSE,
  BFI_O_7 = TRUE,
  BFI_O_8 = TRUE,
  BFI_O_9 = FALSE,
  BFI_O_10 = FALSE,
  BFI_O_11 = TRUE,
  BFI_O_12 = TRUE
)
```

```{r}
ItemsToBeReversed =  names(Reversals)[unlist(Reversals)] #uses the boolean map above to select the column names which = TRUE

baseline_df = baseline_df |> 
  # 1. create *new* columns for the reversed items, suffixing them with "r"
  mutate(across(all_of(ItemsToBeReversed),
                ~ 6 - .x,
                .names = "{.col}r")) %>%
  # 2. drop the *original* (un-suffixed) reversed columns
  select(-all_of(ItemsToBeReversed ))

baseline_df |> select(contains("BFI_E")) |> cor(use='pairwise.complete.obs') |> round(2)
```

#Latent Variables or Composites for Big Five Traits?

```{r}
#something like: baseline_df |> select(contains("BFI_E")) |> rowMeans() etc. 
```
#Exploratory Visualization


```{r}
ema_df |>  group_by(SID) %>%
  filter(n() > 40) %>%
  ungroup() %>%
  filter(SID %in% sample(unique(.$SID), 20)) %>%
  ggplot(aes(x = Session, y = Dsm_score)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth() + 
    facet_wrap(~SID, nrow = 4) + 
    theme_bw()
```

5.  R0 (Descriptives, Reliabilities)
    a.  reliabilities, means, sd, corr matrices, baseline descriptives).
    b.  Use omega (use multilevel omega c.f. Colin)
    c.  Use Evan's descriptives function?
    
EW note: this aren't finalized; once we get this sorted out we can either a) feed to Kable or b) feed to the Officer/Flextable function above.
```{r}
CurrentDateString
warning("These are descriptives for the EMA data. I assume we do no pooling within individuals first.")

BasicStatsEma = ema_df |> select(Nback_score:Dsm_medianRTc,
                E_assert:Open) |> 
                psych::describe() |> 
                as.data.frame() |> #psych helpfully returns a non-dataframe object
                select(-c(vars,trimmed,mad,skew,kurtosis,se)) |> 
                 rownames_to_column(var = "Variable")

BasicStatsEma
```

```{r}
#now, export to word
FormatTableToWord(BasicStatsEma,paste0("CT_CogB5_BasicStatsEMA_",CurrentDateString,".docx"))
```

```{r}
CorrTable = ema_df |> select(Nback_score:Dsm_medianRTc,
                E_assert:Open) |> cor(use='pairwise.complete.obs') |> round(2)

warning('Need to figure out how to format this for MS Word.')
#Option 1: Outcomes w/ Outcomes, Pers with Pers.?
#Option 2: Break it up over multiple pages. 
CorrTable 
```
#PAUSED HERE (EW)
```{r}
ema_df |> select(starts_with('E_')) |> omega(nfactors = 1)
omega
?omega
ema_df
```
```{r}

#this is an old function I have from a different script.
#omega_total = if (ncol(subset_data) >= 3) {
#      omega(subset_data,nfactors = NumberOfFacets, check.keys=FALSE)$omega.tot
#    } else {
#      NA
#    }
#    
#    # Cronbach's alpha
#    alpha_reliability = if (ncol(subset_data) >= 2) {
#      alpha(subset_data,check.keys=FALSE)$total$raw_alpha
#    } else {
#      NA
#    }
#    
#    # Item-total correlations
#    item_total_cor = if (ncol(subset_data) >= 2) {
#      apply(subset_data, 2, function(x) cor(x, rowSums(subset_data), use = "complete.obs"))
#   } else {
#      rep(NA, ncol(subset_data))
#    }
#    
#    # Add omega, alpha, item-total correlations, and person_wave info to descriptives data frame
#    descriptives = descriptives %>%
#      mutate(
#        omega_total = omega_total,
#        alpha_reliability = alpha_reliability,
#        item_total_cor = item_total_cor,
#        person_wave = paste0(prefix, wave)
#      )
#    

```
# OLDER STUFF

##Code Emorie used to pull the data?

```{r}
pp_cb = readxl::read_excel("Codebooks/01-codebook.xlsx", sheet = "codebook")
b5_items = pp_cb %>% filter(Inventory == "BFI-2") %>% pull(old)
# sc_df = sc_df %>%
#   #rename(N05 = N5) %>%
#   select(SID, Date = Date_main, session:HourBlock, interrupt, all_of(b5_items)) %>%
#   pivot_longer(
#     cols = all_of(b5_items)
#     , values_to = "value"
#     , names_to = "old"
#     , values_drop_na = T
#   ) %>%
#   left_join(pp_cb %>% select(old, shortFacet, Reverse)) %>%
#   mutate(value = ifelse(Reverse == "yes", 6 - value, value)) %>%
#   separate(old, c("trait", "item"), sep = -2) %>%
#   group_by(SID, Date, StartDate, Day, Hour, HourBlock, session, interrupt, trait, shortFacet) %>%
#   summarize(value = mean(value, na.rm = T)) %>% 
#   ungroup()

#pomp = function(x, mini = 1, maxi = 5) (x - mini)/(maxi - mini)*100

sc_b5 = sc_df %>% 
  group_by(SID, Date, StartDate, Day, Hour, HourBlock, session,  interrupt, trait) %>% 
  summarize(value = mean(value, na.rm = T)) %>% 
  ungroup() %>%
  pivot_wider(
    names_from = "trait"
    , values_from = "value"
    ) %>%
  full_join(
    sc_b5 %>%
      pivot_wider(
        names_from = c("trait", "shortFacet")
        , values_from = "value"
        )
  ) #%>% 
  # mutate_at(vars(A:N_emoVol), pomp)
```

## Cognitive States

```{r}
#pomp_obs = function(x) (x - min(x, na.rm = T))/(max(x, na.rm = T)-min(x, na.rm = T))*100
# sc_cog = sc_df %>% 
#   select(
#     SID, Date = Date_main, session:HourBlock, interrupt, 
#          , nback_score, vpa_score, dsm_score
#          , nback_medianRTc, vpa_medianRTc, dsm_medianRTc
#          ) %>%
  # mutate_at(vars(nback_score, vpa_score, dsm_score), pomp_obs) %>%
  # rowwise() %>%
  # mutate(cog_comp = mean(c_across(nback_score:dsm_score), na.rm = T)) %>%
  ungroup() 

sc_b5cog = sc_b5 %>%
  inner_join(
    sc_cog 
    ) %>%
  select(-StartDate, -HourBlock) %>%
  arrange(SID, Date) %>%
  group_by(SID) %>%
  mutate(
    beep = 1:n()
    , beep_wpc = beep - median(beep)
    , dec_time = hour(Date) + minute(Date)/60 + second(Date)/60/60
    , dec_time_c = dec_time - 12
  )
save(sc_b5cog, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-ema-%s.RData", Sys.Date()))
write_csv(sc_b5cog, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-ema-%s.csv", Sys.Date()))
```

```{r}
set.seed(420)
sc_cog %>%
  group_by(SID) %>%
  filter(n() > 40) %>%
  ungroup() %>%
  filter(SID %in% sample(unique(.$SID), 20)) %>%
  ggplot(aes(x = session, y = dsm_score)) + 
    geom_line() + 
    geom_point() + 
    geom_smooth() + 
    facet_wrap(~SID, nrow = 4) + 
    theme_bw()
  
```

#Other Descriptives
Descriptives

```{r}
#make it long
cog_long = cog_wide %>%
  select(SID, Sociability:CrtvImagination) %>%
  pivot_longer(
    cols = Sociability:CrtvImagination
    , names_to = c("trait")
    , values_to = "value"
  ) 

# function for mean, sd, median, min, max, n, n missing
descriptive_fun = function(df, var) {
  df %>%
  summarize(
      mean = mean({{ var }},   na.rm = TRUE),
      sd     = sd({{ var }},     na.rm = TRUE),
      median = median({{ var }}, na.rm = TRUE),
      min    = min({{ var }},    na.rm = TRUE),
      max    = max({{ var }},    na.rm = TRUE),
      # omega  = omega({{var}}),
      # alpha  = alpha({{var}}),
      n      = n(),
      .groups = "drop"
      )
}

cog_descriptives = cog_long %>%
  group_by(SID, trait) %>% # we want descriptive for each trait for each participant
  descriptive_fun(var = value) %>% 
  ungroup()

```

Look at low variance participant-variables

```{r}
cog_descriptives %>%
  filter(sd == 0 ) 
#LETS FUCKING GO
```

Add study variable
```{r}
cog_wide = cog_wide %>%
  mutate(study = "Soc-Cog")

table(is.na(cog_wide))



#post ids for demographics
cog_ids_post = unique(cog_wide$SID)

save(cog_ids_post, file = "Data/cog_ids_post.RData")
```
# Baseline Data

## Big Five Traits

```{r}
bl_df = read_csv("parsed-data/baseline/baseline_wide_clean_data_W1_2025-04-30.csv") %>%
  select(
    SID, StartDate = date
    , starts_with("demo_eth"),
    , demo_gender, demo_gender_TEXT = demo_gender_3_TEXT, demo_hispanic
    , demo_orientation, demo_orientation_TEXT = demo_orientation_4_TEXT
    , demo_YOB, demo_age
    , demo_education
    , demo_income
    , demo_relationship 
    # , demo_livingSituation
    # , demo_employment 
    # , demo_mom_edu, demo_dad_edu
    , starts_with("BFI")
    # , health_physAct = `health-physAct` 
    # , health_smoke = `health-smoke`
    # , health_height = `health-height`, health_weight = `health-weight`
    # , health_cholesterol = `health-CC_1`
    # , health_hypertension = `health-CC_2`
    # , health_diabetes = `health-CC_3`
    # , health_depression = `health-CC_4`
    # , health_tbi = `health-CC_5`
    # , health_heartDis = `health-CC_6`
    # , health_renal = `health-CC_7`
    # , starts_with("sud")
    ) %>%
  filter(!is.na(SID))
write_csv(bl_df, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-baseline-%s.csv", Sys.Date()))
save(bl_df, file = sprintf("data-pulls/b5-cog-ctsem/b5-cog-baseline-%s.RData", Sys.Date()))
```

## Ctsem shite

```{r, include = FALSE}
library(rstan)
library(ctsem)
library(tidybayes)
library(furrr)
library(plyr)
library(tidyverse)
```

## Testing

simple ass test

```{r}
model=ctModel(type='stanct'
            , n.latent = 2, latentNames   = c("E", "vpa_score")
            , n.manifest = 2, manifestNames = c("E", "vpa_score")
            , TIpredNames = c("E_baseline", "age")
            , TDpredNames = c("interrupt", "Hour", "session")
            , LAMBDA=diag(2)
            , MANIFESTMEANS = 0 #fix this for estimation 
            , CINT = matrix(c("mu_trait", "mu_cog"), nrow = 2) #estimate this for equil calculation
            , DRIFT = matrix(c("beta_trait", "beta_trait_on_cog", "beta_cog_on_trait", "beta_cog"), nrow = 2, byrow = T)
            #diagonal for estimation
            , MANIFESTVAR   = matrix(c("mv_trait", 0, 0, "mv_cog"), nrow = 2, byrow = TRUE)
            , DIFFUSION = matrix(c("diff_trait", 0, 0, "diff_cog"), nrow = 2, byrow = TRUE), 
            , T0VAR = matrix(c("var_trait", 0, 0, "var_cog"), nrow = 2, byrow = TRUE) 
            , id            = "id"
            , time          = "time" )

model$pars$indvarying[7:14] = TRUE #drift and diffusion
model$pars$transform[11] = "log1p_exp(2 * param) + 1e-10" #drift
model$pars$transform[14] = "log1p_exp(2 * param) + 1e-10" #diffusion
model$pars$transform[21:22] = "1*param" #CINT

model$pars



model_fit = ctStanFit(datalong = sherman_wide_person_centered, 
                      ctstanmodel = model,
                      optimize = TRUE,
                      priors = TRUE,
                      cores=10
                      # ,
                      # optimcontrol=list(is=TRUE, 
                      #                   finishsamples = 1000, 
                      #                   isloopsize = 2000)
                      )

# Summary of the model
modelsum = summary(model_fit, digits = 4, parmatrices=TRUE)
modelpar = modelsum$parmatrices
print(modelsum$parmatrices)

```

All 30 trait-cog pairs

```{r}
# Define traits and cogs
traits = colnames(sc_df)[4:8] #change col numbers
cogs = colnames(sc_df)[9:16] #change col numbers

# Create all trait-cog pairs
trait_cog_pairs = expand_grid(trait = traits, cog = cogs)

fit_bivariate_model = function(trait, cog) {
  message("Fitting model for:", trait, "and", cog, "\n")

  tryCatch({
    var_names = c(trait, cog)

    model = ctModel(
      type = 'stanct',
      n.latent = 2,
      latentNames = var_names,
      n.manifest = 2,
      manifestNames = var_names,
      LAMBDA = diag(2),
      
      TDpred
      
      MANIFESTMEANS = 0,

      CINT = matrix(c(
        paste0("mu_", trait),
        paste0("mu_", cog)
      ), nrow = 2),

      DRIFT = matrix(c(
        paste0("beta_", trait),
        paste0("beta_", trait, "_on_", cog),
        paste0("beta_", cog, "_on_", trait),
        paste0("beta_", cog)
      ), nrow = 2, byrow = TRUE),

      MANIFESTVAR = matrix(c(
        paste0("mv_", trait), 0,
        0, paste0("mv_", cog)), 
        nrow = 2, byrow = TRUE),

      DIFFUSION = matrix(c(
        paste0("diff_", trait), 0,
        0, #lower diagonal f's stuff up
        paste0("diff_", cog)
      ), nrow = 2, byrow = TRUE),

      T0VAR = matrix(c(
        paste0("t0var_", trait), 0,
        0, paste0("t0var_", cog)), 
        nrow = 2, byrow = TRUE),

      id = "id",
      time = "time"
    )

    model$pars$indvarying[7:14] = TRUE #drift and diffusion
    
    #make diffusion variances reasonable
    model$pars$transform[11] = "log1p_exp(2 * param) + 1e-10"
    model$pars$transform[14] = "log1p_exp(2 * param) + 1e-10"

    data_sub = sherman_wide_person_centered %>%
      select(id, time, all_of(c(trait, cog)))

    fit = ctStanFit(datalong = data_sub, 
                     ctstanmodel = model, 
                     optimize = TRUE, 
                     priors = TRUE,
                     cores = 2, 
                     verbose = 1
                     )

    fit_summary = summary(fit, digits = 4, parmatrices = TRUE)

    tibble(
      pair = paste0(trait, "_", cog),
      model = list(model),
      fit = list(fit),
      summary = list(fit_summary)
    )
  }, error = function(e) {
    cat("Error in model for: ", trait, "_", cog, "\n", e$message)  # Log the error
    tibble(
      pair = paste0(trait, "_", cog),
      model = list(NA),
      fit = list(NA),
      summary = list(e$message)
    )
  })
}

# Parallel plan
plan(multisession, workers = 4)

# Fit all models
ml_bivariate_results = future_pmap_dfr(trait_cog_pairs, fit_bivariate_model, .progress = TRUE)
#check
# ml_bivariate_results$summary[3][[1]]$parmatrices

# Save results
save(ml_bivariate_results, file = "Data/ml_bivariate_results.RData")

# Return to sequential plan
plan(sequential)

q()

```

Extract parameters and find out which didn't converge and reestimate

Yo we good - 5/14/25

```{r}
# #load("Data/ml_bivariate_results.RData")
# 
# ml_bivariate_summary_t0_fixed = ml_bivariate_results %>%
#   select(pair, summary)
# 
# # Extract the parmatrices from each summary
# ml_bivariate_parmatrices_t0_fixed = ml_bivariate_summary_t0_fixed %>%
#   mutate(parmatrices = map(summary, ~ .x$parmatrices)) %>%
#   select(pair, parmatrices)
# 
# #Are any estimates hella big; this is  indicative of poor model convergence and I can separately estimate them
# 
# par_check_t0_fixed = ml_bivariate_parmatrices_t0_fixed %>%
#   mutate(any_mean_gt_20 = map_lgl(parmatrices, ~ any(.x$Mean > 20, na.rm = TRUE)))
# 
# par_large_means_t0_fixed = par_check_t0_fixed %>%
#   filter(any_mean_gt_20) %>%
#   mutate(high_mean_rows = map(parmatrices, ~ filter(.x, Mean > 20)))
# 
# par_large_unnest_t0_fixed = par_large_means_t0_fixed %>% unnest(high_mean_rows) %>%
#   mutate(Mean = round(Mean, 3))
# 
# #im so fuckin happy
```
